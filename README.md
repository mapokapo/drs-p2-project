# P2 - Distribuirana koordinacija (Tim T2)

**Studij:** Diplomski studij raÄunarstva  
**AWS Academy Class:** P2 - Distribuirani raÄunalni sustavi  
**VeliÄina tima:** 4 Älana

---

## ğŸ“‹ SaÅ¾etak projekta

Ovaj repozitorij sadrÅ¾i **kompletan distribuirani sustav** implementiran na AWS platformi koji demonstrira kljuÄne algoritme koordinacije u distribuiranim sustavima. Projekt u potpunosti zadovoljava zahtjeve projektnog zadatka P2 i koristi **Terraform** za _Infrastructure as Code_ (IaC) s potpuno automatiziranim postavljanjem okruÅ¾enja.

### Implementirani algoritmi i koncepti:

1. **Lamportovi logiÄki satovi** - LogiÄko mjerenje vremena bez globalnog sata
2. **Ricart-Agrawala algoritam** - MeÄ‘usobno iskljuÄivanje (Mutex) za pristup kritiÄnoj sekciji
3. **Bully algoritam** - Izbor voÄ‘e (Leader Election) i automatski oporavak od kvara

### Ispunjeni minimalni zahtjevi:

âœ… **5 Ävorova** - Svaki Ävor s jedinstvenim identitetom  
âœ… **Lamportov sat** - Ispravna implementacija `max + 1` pravila  
âœ… **MeÄ‘usobno iskljuÄivanje** - Dokazano u CloudWatch logovima  
âœ… **Izbor voÄ‘e** - Automatski oporavak od kvara voÄ‘e (heartbeat + timeout)  
âœ… **Mjerenja performansi** - Broj poruka i vrijeme Äekanja  
âœ… **CloudWatch** - Strukturirani logovi i alarmi  
âœ… **IAM** - Least-privilege princip (LabInstanceProfile)  
âœ… **Tagiranje** - `Project=P2` i `Team=T2`

---

## ğŸ‘¥ Struktura tima

- **Vedran MariÄ‡** - Voditelj projekta, integracija, dokumentacija i priprema demo-a
- **AnÄ‘ela MarinoviÄ‡** - Komunikacija i infrastruktura (AWS/Terraform, bootstrap)
- **Leo PetroviÄ‡** - LogiÄko vrijeme i meÄ‘usobno iskljuÄivanje (Lamport sat, Ricart-Agrawala)
- **Nikola Pehar** - Izbor voÄ‘e i mjerenja (Bully algoritam, eksperimenti, analiza)

---

## ğŸ“‚ Struktura projekta

```
â”œâ”€â”€ src/                      # Python kod Ävorova
â”‚   â”œâ”€â”€ node.py              # Glavna implementacija (Lamport, Ricart-Agrawala, Bully)
â”‚   â”œâ”€â”€ cloudwatch_logger.py # CloudWatch logging integracija
â”‚   â”œâ”€â”€ peers.json           # Konfiguracija peer mreÅ¾e (generira Terraform za AWS)
â”‚   â”œâ”€â”€ pyproject.toml       # Python dependencies (uv format)
â”‚   â””â”€â”€ uv.lock              # Locked dependencies
â”œâ”€â”€ benchmark/               # Mjerenje performansi
â”‚   â”œâ”€â”€ benchmark.py         # Skripta za automatsko mjerenje performansi
â”‚   â”œâ”€â”€ peers_3nodes.json    # 3-node konfiguracija za benchmark
â”‚   â”œâ”€â”€ peers_5nodes.json    # 5-node konfiguracija za benchmark
â”‚   â””â”€â”€ peers_7nodes.json    # 7-node konfiguracija za benchmark
â”œâ”€â”€ terraform/               # AWS infrastruktura (IaC)
â”‚   â”œâ”€â”€ main.tf              # Terraform konfiguracija (VPC, EC2, IAM, deploy)
â”‚   â””â”€â”€ user_data.sh.tpl     # Bootstrap skripta za EC2 instance
â”œâ”€â”€ scripts/                 # Admin i deploy skripte
â”‚   â”œâ”€â”€ admin_script.sh      # Slanje komandi na remote Ävorove via tmux
â”‚   â””â”€â”€ deploy.sh            # Deploy i pokretanje Ävora (poziva Terraform)
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.md      # Arhitekturni dijagram (Mermaid format)
â”œâ”€â”€ .gitignore               # Git ignore pravila
â””â”€â”€ README.md                # Ovaj dokument

```

---

## ğŸ—ï¸ Arhitektura sustava

> ğŸ“Š **Vizualni dijagrami:** Detaljna arhitektura s Mermaid dijagramima dostupna je u [`docs/architecture.md`](docs/architecture.md)

### Infrastruktura (AWS)

Sustav se sastoji od **5 EC2 instanci** (t3.micro, Ubuntu 24.04) unutar default VPC-a:

- **VPC i Subnet** - Koristi default VPC s automatskim izborom subneta
- **Security Group** - Dozvoljava SSH (port 22) i inter-node TCP komunikaciju (port 5000)
- **IAM Instance Profile** - `LabInstanceProfile` za CloudWatch pristup
- **CloudWatch Logs** - Log grupa `/Distributed_System_Logs` sa streamovima `Node_1` do `Node_5`
- **Tagiranje** - Sve instance imaju `Project=P2` i `Team=T2` tagove

### Komunikacija

- **TCP socketi** - Length-prefixed JSON poruke preko perzistentnih konekcija
- **Protokol** - 6 tipova poruka: `REQUEST`, `REPLY`, `ELECTION`, `ANSWER`, `COORDINATOR`, `HEARTBEAT`
- **Peer discovery** - Konfiguracijska datoteka `peers.json` (ID â†’ IP:Port mapping)
- **Failure detection** - Timeouts i automatsko oznaÄavanje neaktivnih Ävorova

### Algoritmi

#### 1. Lamportovi logiÄki satovi

- Svaka poruka nosi timestamp
- Pri slanju: `clock += 1`
- Pri primanju: `clock = max(local_clock, received_clock) + 1`

#### 2. Ricart-Agrawala Mutex

- 3 stanja: `RELEASED`, `WANTED`, `HELD`
- ÄŒvor Å¡alje `REQUEST` svim peerima
- Ulazak u kritiÄnu sekciju tek kad pristignu svi `REPLY` odgovori
- Deferred replies - Ävor odgaÄ‘a odgovor ako je u `HELD` ili `WANTED` stanju s manjim timestampom

#### 3. Bully Leader Election

- NajviÅ¡i ID postaje koordinator
- Heartbeat poruke svakih 2s
- Timeout od 5s pokreÄ‡e izbor
- `ELECTION` poruke Å¡alju se viÅ¡im ID-ovima
- Ako nema `ANSWER`, Ävor postaje koordinator i Å¡alje `COORDINATOR` poruke

---

## ğŸš€ Upute za pokretanje

### Preduvjeti

1. **AWS Academy Learner Lab** - Pristup aktivnom Lab okruÅ¾enju
2. **Terraform** - Instaliran lokalno ([download](https://www.terraform.io/downloads))
3. **SSH kljuÄ** - `labsuser.pem` kopiran u `~/.ssh/` s pravima `chmod 400`
4. **AWS kredencijali** - Preuzeti iz AWS Academy _AWS Details_ panela

### Konfiguracija AWS kredencijala

Iz AWS Academy Learner Lab suÄelja:

1. Kliknite **AWS Details** â†’ **Show** (desno od _AWS CLI_)
2. Kopirajte kredencijale u `~/.aws/credentials`:

```ini
[default]
aws_access_key_id = <VAÅ _ACCESS_KEY>
aws_secret_access_key = <VAÅ _SECRET_KEY>
aws_session_token = <VAÅ _SESSION_TOKEN>
```

### 1. Pokretanje infrastrukture (Deploy)

```bash
cd terraform
terraform init
terraform apply -auto-approve
```

**Trajanje:** 2-3 minute za kreiranje instanci + instalaciju paketa + deploy aplikacije.

**Å to se dogaÄ‘a:**

1. Terraform kreira 5 EC2 instanci s pripadajuÄ‡om mreÅ¾nom infrastrukturom
2. `user_data.sh.tpl` instalira Python 3 i boto3
3. `terraform_data.node_deployment` kopira `node.py`, `cloudwatch_logger.py`, i `peers.json`
4. `deploy.sh` pokreÄ‡e svaki Ävor u tmux sesiji s `USE_CLOUDWATCH=true`

**Output:**

```
node_ips = {
  "Node 1" = "54.123.45.67"
  "Node 2" = "54.123.45.68"
  ...
}
ssh_quickstart = "ssh -i ~/.ssh/labsuser.pem ubuntu@54.123.45.67"
```

### 2. Monitoring (CloudWatch Logs)

#### Pregled logova:

1. AWS Console â†’ **CloudWatch** â†’ **Logs** â†’ **Log groups**
2. Odaberite grupu: `/Distributed_System_Logs`
3. Odaberite stream po Ävoru: `Node_1`, `Node_2`, ..., `Node_5`

#### Å to traÅ¾iti u logovima:

| Tip dogaÄ‘aja     | Opis                        | Primjer                                |
| ---------------- | --------------------------- | -------------------------------------- |
| `CLOCK_UPDATE`   | Lamport sat se aÅ¾urira      | `"clock": 42, "received_time": 41`     |
| `SEND_MESSAGE`   | Slanje poruke               | `"type": "REQUEST", "target": 3`       |
| `ENTER_CS`       | Ulazak u kritiÄnu sekciju   | `"state": "HELD", "request_clock": 15` |
| `EXIT_CS`        | Izlazak iz kritiÄne sekcije | `"deferred_replies": [2, 4]`           |
| `LEADER_UPDATE`  | Nova voÄ‘a                   | `"coordinator_id": 5`                  |
| `NODE_DOWN`      | Detektirani kvar            | `"target": 3, "reason": "timeout"`     |
| `ELECTION_START` | PoÄetak izbora voÄ‘e         | `"reason": "heartbeat_timeout"`        |

#### CloudWatch Insights primjeri:

**Broj zahtjeva za mutex po Ävoru:**

```
fields @timestamp, node_id, event_type
| filter event_type = "ENTER_CS"
| stats count() by node_id
```

**Vrijeme u kritiÄnoj sekciji:**

```
fields @timestamp, node_id
| filter event_type = "ENTER_CS" or event_type = "EXIT_CS"
| sort @timestamp asc
```

### 3. Demo scenariji i testiranje

#### Scenario 1: Paralelni zahtjevi za kritiÄnu sekciju

ÄŒvorovi automatski generiraju zahtjeve. Provjerite u CloudWatch logovima da **nikad dva Ävora istovremeno nisu u kritiÄnoj sekciji**.

#### Scenario 2: Simulacija kvara voÄ‘e

1. Spojite se na instancu trenutnog koordinatora:

   ```bash
   ssh -i ~/.ssh/labsuser.pem ubuntu@<KOORDINATOR_IP>
   ```

2. PronaÄ‘ite proces i ubijte ga:

   ```bash
   ps aux | grep node.py
   sudo kill <PID>
   ```

3. U CloudWatch logovima ostalih Ävorova pratite:
   - Prestanak `HEARTBEAT` poruka
   - `ELECTION_START` dogaÄ‘aj nakon timeoutĞ° (5s)
   - `ELECTION` i `ANSWER` poruke
   - `LEADER_UPDATE` s novim `coordinator_id`

#### Scenario 3: KoriÅ¡tenje admin skripte

Za slanje komandi na Ävorove bez SSH-a:

```bash
cd scripts
./admin_script.sh 1 elect   # Pokreni izbor na Node 1
./admin_script.sh 2 req     # ZatraÅ¾i mutex na Node 2
./admin_script.sh 3 status  # PrikaÅ¾i status Node 3
```

### 4. Mjerenja performansi

#### Automatski benchmark

Za automatsko mjerenje performansi preko 3 konfiguracije (3, 5 i 7 Ävorova):

```bash
cd benchmark
python3 benchmark.py          # 5 zahtjeva po konfiguraciji (default)
python3 benchmark.py 10       # 10 zahtjeva po konfiguraciji
```

**Skripta automatski:**

1. PokreÄ‡e Ävorove za svaku konfiguraciju
2. Triggerira izbor voÄ‘e
3. Å alje mutex zahtjeve
4. Analizira logove
5. Generira izvjeÅ¡taje

**Izlazni fajlovi:**

- `benchmark_results.json` - Sirovi podaci u JSON formatu
- `benchmark_report.md` - Markdown izvjeÅ¡taj s tablicama i analizom

**Primjer izlaza:**

```
==========================================================================================
FINAL RESULTS TABLE
==========================================================================================
Config                Nodes   CS Entries   REQ Msgs   REPLY Msgs   Avg Wait   Max Wait
------------------------------------------------------------------------------------------
3-node cluster            3            5         10           10      0.500s      0.600s
5-node cluster            5            5         20           20      1.000s      1.200s
7-node cluster            7            5         30           30      1.400s      1.600s
------------------------------------------------------------------------------------------

Message complexity analysis (Ricart-Agrawala: 2(N-1) messages per CS request):
  3 nodes: Expected 4 msgs/request
  5 nodes: Expected 8 msgs/request
  7 nodes: Expected 12 msgs/request
==========================================================================================
```

#### RuÄna analiza (CloudWatch)

ÄŒvorovi logiraju:

- **Broj poruka** - `SEND_MESSAGE` dogaÄ‘aji s brojem poslatih poruka
- **Vrijeme Äekanja** - Razlika izmeÄ‘u `REQUEST` i `ENTER_CS` timestampova

**Konfigurirane konfiguracije za testiranje:**

1. Normalan rad (5 Ävorova)
2. Kvar voÄ‘e (4 aktivna Ävora)
3. Visoka konkurencija (uÄestali mutex zahtjevi)

**Analiza u CloudWatch Insights:**

```
fields @timestamp, node_id, event_type, details.wait_time_ms
| filter event_type = "MUTEX_STATS"
| stats avg(details.wait_time_ms) as avg_wait, max(details.wait_time_ms) as max_wait by node_id
```

---

## ğŸ’» Lokalno pokretanje (Development)

Za razvoj i testiranje bez AWS infrastrukture:

### 1. Priprema okruÅ¾enja

```bash
cd src
# Osigurajte da imate Python 3.10+
python3 --version
```

### 2. Konfiguracija

Datoteka `src/peers.json` veÄ‡ sadrÅ¾i lokalnu konfiguraciju:

```json
{
  "1": { "ip": "127.0.0.1", "port": 5001 },
  "2": { "ip": "127.0.0.1", "port": 5002 },
  "3": { "ip": "127.0.0.1", "port": 5003 },
  "4": { "ip": "127.0.0.1", "port": 5004 },
  "5": { "ip": "127.0.0.1", "port": 5005 }
}
```

### 3. Pokretanje Ävorova

U **5 zasebnih terminala**:

```bash
# Terminal 1
cd src
python3 node.py --id 1 --peers peers.json

# Terminal 2
cd src
python3 node.py --id 2 --peers peers.json

# ... i tako dalje do Node 5
```

**Napomena:** U lokalnom naÄinu rada `USE_CLOUDWATCH` je automatski `False` i logovi se ispisuju na stdout u JSON formatu.

### 4. Interakcija

U terminalima Ävorova moÅ¾ete upisivati komande:

- `req` - ZatraÅ¾i ulazak u kritiÄnu sekciju
- `elect` - Pokreni izbor voÄ‘e
- `status` - PrikaÅ¾i trenutno stanje Ävora
- `quit` - Zaustavi Ävor

---

## ğŸ§¹ ÄŒiÅ¡Ä‡enje infrastrukture (Cleanup)

**VAÅ½NO:** Nakon zavrÅ¡etka testiranja obavezno uklonite sve AWS resurse da ne troÅ¡ite kredit!

```bash
cd terraform
terraform destroy -auto-approve
```

**Å to se briÅ¡e:**

- Sve EC2 instance (Node-1 do Node-5)
- Security Group (dist-system-sg)
- CloudWatch log streamovi (log grupa ostaje)

**Dodatno ruÄno brisanje (opcionalno):**

- CloudWatch Log Group `/Distributed_System_Logs` (AWS Console â†’ CloudWatch â†’ Log groups)

---

## ğŸ“Š Sigurnost i najbolje prakse

### Sigurnost

âœ… **IAM Least Privilege** - Koristi se postojeÄ‡i `LabInstanceProfile` s minimalnim potrebnim dozvolama  
âœ… **Security Groups** - OgraniÄena komunikacija samo na potrebne portove  
âœ… **Tajne** - Nisu pohranjene u kodu; koriste se environment varijable  
âœ… **SSH** - Pristup samo s privatnim kljuÄem (`labsuser.pem`)

### Toleriranje kvarova

âœ… **Failure Detection** - TCP timeout + retry mehanizam  
âœ… **Dead Node Tracking** - Thread-safe praÄ‡enje neaktivnih Ävorova  
âœ… **Leader Recovery** - Automatski heartbeat i re-election  
âœ… **Mutex Resilience** - Smanjeni quorum ako Ävor nije dostupan

### Reproducibilnost

âœ… **IaC** - Terraform konfiguracija s verzioniranim stanjem  
âœ… **Automatizacija** - Potpuno automatski deploy od nule do pokretanja  
âœ… **Dokumentacija** - Jasne upute za setup, test i teardown  
âœ… **Git** - Verzioniranje koda i infrastrukture

---

## ğŸ“š Dodatne informacije

### Struktura poruka (JSON)

```json
{
  "sender": 3,
  "type": "REQUEST",
  "timestamp": 42,
  "request_clock": 42
}
```

### Lamport Clock pravila

```python
# Slanje poruke
def tick():
    clock += 1
    return clock

# Primanje poruke
def update_clock(received_time):
    clock = max(clock, received_time) + 1
```

### Ricart-Agrawala Mutex algoritam

```
1. ÄŒvor prelazi u WANTED i Å¡alje REQUEST(clock) svim peerima
2. Peer odgovara REPLY odmah AKO:
   - Je u RELEASED stanju, ILI
   - Je u WANTED ali ima veÄ‡i (clock, node_id) par
3. ÄŒvor ulazi u HELD kad skupi sve REPLY odgovore
4. Pri izlasku Å¡alje REPLY svim odgoÄ‘enim requestovima
```

### Bully Election algoritam

```
1. Heartbeat prestane â†’ election_timeout (5s) istekne
2. ÄŒvor Å¡alje ELECTION viÅ¡im ID-ovima
3. AKO dobije ANSWER â†’ Äeka COORDINATOR
4. AKO ne dobije ANSWER â†’ proglaÅ¡ava se koordinatorom
5. Novi koordinator Å¡alje COORDINATOR svima
```

---

## ğŸ”— Reference

- **Projektni zadatak:** `task.md`
- **Terraform dokumentacija:** https://www.terraform.io/docs
- **AWS Academy:** AWS Academy Learner Lab upute
- **Lamport Clocks:** Leslie Lamport, "Time, Clocks, and the Ordering of Events in a Distributed System"
- **Ricart-Agrawala:** G. Ricart & A.K. Agrawala, "An Optimal Algorithm for Mutual Exclusion"
- **Bully Algorithm:** H. Garcia-Molina, "Elections in a Distributed Computing System"

---

## ğŸ“ Licenca i autori

**Projekt:** P2 - Distribuirana koordinacija  
**Kolegij:** Distribuirani raÄunalni sustavi  
**Akademska godina:** 2025/2026  
**Tim:** T2

Svi Älanovi tima su ravnopravno doprinijeli projektu prema definiranim ulogama.
